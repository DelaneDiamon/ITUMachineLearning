\documentclass{article}
\title{Naive Bayes \& K-Nearest Neighbor Homework}
\author{Dmitrii Dunin, ITU ID 94739}
%\DeclareGraphicsExtensions{.png,.pdf}
\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=Dmitrii}
\begin{center}
{\bf\Large Naive Bayes \& K-Nearest Neighbor}
\end{center}
\begin{center}
{\Large Dmitrii Dunin, ITU ID 94739}
\end{center}
\begin{center}
{\bf\Large International Technological University}
\end{center}

\section*{Question 1}

<<label=Setup1, echo=True,results=verbatim>>=
rm(list=ls())
setwd("F:/Workspace/R/Homework5")
par(mar=rep(2, 4))
wine_data <- read.table("winequality-red.csv", header=TRUE, sep=";")
wine_train = wine_data[1:1400,]
wine_test = wine_data[1401:1599,]
@

1) In homework 3 ridge regression was performed on the wine quality data
set. Now use k-nearest neighbors to classify this data. Use cross validation
to choose the best value for k. Round the results from the ridge regression to
the nearest integer to form the classification with ridge regression. Using the
best values for k (nearest neighbor) and lambda (ridge regression), compare
and contrast the results of these two classification techniques.

\section*{Answer  1}
<<label=Answer1, echo=True,results=verbatim>>=
wine_train_cv = wine_train

rmse = function(X, Y)
{
  return(sqrt(sum((X - Y) ^ 2) / length(Y)))
}

library(MASS)

xlambda = rep(0, times = 30)
for (i in seq(from = 0, to = 29))
{
  exp <- (+3 - 4 * (i / 20))
  xlambda[i + 1] <- 10 ^ exp
}

cross_valid <-
  function(df_train_cv, k, args, method, err_type = "rmse")
  {
    error_train <- 0
    error_cv <- 0
    num_sample <- nrow(df_train_cv)
    df_train = df_train_cv[1:(num_sample * ((k - 1) / k)), ]
    df_cv = df_train_cv[1:(num_sample * (1 / k)), ]
    pick <- k #pick kth set
    
    for (j in 1:k) {
      i_tmp <- 1
      for (i in 1:k) {
        if (i == pick) {
          df_cv <- df_train_cv[((i - 1) * num_sample / k + 1):(num_sample * (i / k)), ]
        } else {
          df_train[((i_tmp - 1) * num_sample / k + 1):(num_sample * (i_tmp / k)),] <-
            df_train_cv[((i - 1) * num_sample / k + 1):(num_sample * (i / k)), ]
          i_tmp <- i_tmp + 1
        }
        
      }
      
      pick <- pick - 1
      
      y_df_train <- df_train[, ncol(df_train)]
      x_df_train <- df_train[, -ncol(df_train)]
      yx_df_train <- cbind(x_df_train, y_df_train)
      
      
      y_df_cv <- df_cv[, ncol(df_cv)]
      x_df_cv <- df_cv[, -ncol(df_cv)]
      yx_df_cv <- cbind(x_df_cv, y_df_cv)
      
      if (method == "linear ridge") {
        fit <- lm.ridge(y_df_train ~ ., yx_df_train, lambda = xlambda[args])
        A <- as.array(fit$coef[1:(ncol(df_train) - 1)] / fit$scales)
        X <- as.matrix(x_df_train)
        for (i in seq(from = 1, to = ncol(x_df_train))) {
          X[, i] <- X[, i] - fit$xm[i]
        }
        yh <- X %*% A + fit$ym
        error_train <- error_train + rmse(round(yh), y_df_train) / k * 0.1 / 0.1
        X <- as.matrix(x_df_cv)
        for (i in seq(from = 1, to = ncol(x_df_cv))) {
          X[, i] <- X[, i] - fit$xm[i]
        }
        yh <- X %*% A + fit$ym
        error_cv <- error_cv + rmse(round(yh), y_df_cv) / k * 0.1 / 0.1
      }
      
      if (method == "k nearest neighbors") {
        y_df_train <- as.factor(y_df_train)
        
        x_df_train <- scale(x_df_train)
        KNN <- knn(x_df_train, x_df_train, y_df_train, k = xnn[args])
        
        if (err_type == "rmse") {
          error_train <-
            error_train + rmse(as.numeric(as.character(y_df_train)),
                               as.numeric(as.character(KNN))) / k * 0.1 / 0.1
        }
        
        if (err_type == "class") {
          error_train <-
            error_train + (1 - sum(abs(y_df_train == KNN)) / length(KNN)) / k * 0.1 /
            0.1
        }
        
        x_df_cv <- scale(x_df_cv)
        KNN <- knn(x_df_train, x_df_cv, y_df_train, k = xnn[args])
        if (err_type == "rmse") {
          error_cv <-
            error_cv + rmse(y_df_cv, as.numeric(as.character(KNN))) / k * 0.1 / 0.1
        }
        
        if (err_type == "class") {
          error_cv <-
            error_cv + (1 - sum(abs(y_df_cv == KNN)) / length(KNN)) / k * 0.1 / 0.1
        }
      }
    }
    return(c(error_train, error_cv))
  }

k <- 5
wine_train_err <- NULL
wine_cv_err <- NULL
for (ilambda in 1:length(xlambda)) {
  wine_err <-
    cross_valid(wine_train_cv, k, ilambda, method = "linear ridge")
  wine_train_err[ilambda] <- wine_err[1]
  wine_cv_err[ilambda] <- wine_err[2]
}
wine_train_err_lr <- wine_train_err
wine_cv_err_lr <- wine_cv_err
min_lambda_id <- min(which(min(wine_cv_err_lr) == wine_cv_err_lr))
min_wine_lambda <- xlambda[min_lambda_id]
sprintf("%dth lambda %f is optimal.", min_lambda_id, min_wine_lambda)
@
<<label=Answer1plot1, echo=True,results=verbatim, fig=true>>=
plot(1:length(xlambda), wine_train_err_lr, ylim = c(
  min(wine_train_err_lr, wine_cv_err_lr),
  max(wine_train_err_lr, wine_cv_err_lr)
))

points(1:length(xlambda), wine_cv_err_lr, col = 'red')
points(min_lambda_id,
       wine_cv_err_lr[min_lambda_id],
       pch = 19,
       col = "orange")
@
<<label=Answer1cont1, echo=True,results=verbatim>>=
library(class)
require(class)
wine_train_err <- NULL
wine_cv_err <- NULL
xnn <- c(1:15)
for (inn in 1:length(xnn)) {
  wine_err <-
    cross_valid(wine_train_cv, k, inn, method = "k nearest neighbors")
  wine_train_err[inn] <- wine_err[1]
  wine_cv_err[inn] <- wine_err[2]
}

wine_train_err_knn <- wine_train_err
wine_cv_err_knn <- wine_cv_err

margin = 0.04
min_k_id <-
  min(which(min(wine_cv_err_knn) + margin > wine_cv_err_knn))
min_k_id <-
  ifelse(xnn[min_k_id] %% 2, min_k_id, min_k_id + 1) # K must be odd
min_wine_k <- xnn[min_k_id]
sprintf("k = %d is optimal.", min_wine_k)
@
<<label=Answer1plot2, echo=True,results=verbatim, fig=true>>=
plot(1:length(xnn), wine_train_err_knn, ylim = c(
  min(wine_train_err_knn, wine_cv_err_knn),
  max(wine_train_err_knn, wine_cv_err_knn)
))

points(1:length(xnn), wine_cv_err_knn, col = 'orange')
points(min_k_id, wine_cv_err_knn[min_k_id], pch = 19, col = "orange")
@
<<label=Answer1cont2, echo=True,results=verbatim>>=
#Comparing 2 algorithms

y_wine_train_cv <- wine_train_cv[, ncol(wine_train_cv)]
x_wine_train_cv <- wine_train_cv[, -ncol(wine_train_cv)]
yx_wine_train_cv <- cbind(x_wine_train_cv, y_wine_train_cv)
y_wine_test <- wine_test [, ncol(wine_test)]
x_wine_test <- wine_test [, -ncol(wine_test)]
yx_wine_test <- cbind(x_wine_test, y_wine_test)
fit <-
  lm.ridge(y_wine_train_cv ~ ., yx_wine_train_cv, lambda = min_wine_lambda)
A <- as.array(fit$coef[1:(ncol(wine_train_cv) - 1)] / fit$scales)
X <- as.matrix(x_wine_train_cv)
for (i in seq(from = 1, to = ncol(x_wine_train_cv))) {
  X[, i] <- X[, i] - fit$xm[i]
}

yh <- X %*% A + fit$ym
error_wine_train_lr <- rmse(round(yh), y_wine_train_cv) * 0.1 / 0.1
error_wine_train_lr

X <- as.matrix(x_wine_test)
for (i in seq(from = 1, to = ncol(x_wine_test))) {
  X[, i] <- X[, i] - fit$xm[i]
}


yh <- X %*% A + fit$ym
error_wine_test_lr <- rmse(round(yh), y_wine_test) * 0.1 / 0.1
error_wine_test_lr

y_wine_train_cv <- as.factor(y_wine_train_cv)
x_wine_train_cv <- scale(x_wine_train_cv)
KNN <-
  knn(x_wine_train_cv, x_wine_train_cv, y_wine_train_cv, k = min_wine_k)
error_wine_train_knn <-
  rmse(as.numeric(as.character(y_wine_train_cv)), as.numeric(as.character(KNN))) *
  0.1 / 0.1

error_wine_train_knn

x_wine_test <- scale(x_wine_test)
KNN <-
  knn(x_wine_train_cv, x_wine_test, y_wine_train_cv, k = min_wine_k)
error_wine_test_knn <-
  rmse(y_wine_test, as.numeric(as.character(KNN))) * 0.1 / 0.1
error_wine_test_knn 

tb_wine_lr_vs_knn <-
  data.frame(
    "Train err" = c(error_wine_train_lr, error_wine_train_knn),
    "Test err" = c(error_wine_test_lr, error_wine_test_knn)
  )

row.names(tb_wine_lr_vs_knn) <-
  c("Linear Ridge", "K-Nearest Neighbors")
print(tb_wine_lr_vs_knn)
@

\section*{Question 2}

<<label=Setup2, echo=True,results=verbatim>>=
rm(list=ls())
@

2) Use k-nearest neighbors to classify the Iris data set. 
Compare the knearest neighbor results with the results 
obtained in class using the Naive Bayes Classifier.

\section*{Answer  2}

<<label=Answer2, echo=True,results=verbatim>>=
library(class)
library(e1071)
iris = as.data.frame(iris)
str(iris)
train <- iris[, 1:4]
labels <- iris[, 5]
###Scale the data
train2 <- train
for (i in seq(from = 1, to = ncol(train))) {
  v = var(train[, i])
  m = mean(train[, i])
  train2[, i] <- (train[, i] - m) / sqrt(v)
}
####Perform cross validation on the new data
out <- knn.cv(train2, labels, k = 3)
1 - sum(abs(labels == out)) / length(out)
## [1] 0.05333333
Err <- rep(0, 50)
for (kk in seq(from = 1, to = 50)) {
  out <- knn.cv(train2, labels, k = kk)
  Error <- 1 - sum(abs(labels == out)) / length(out)
  Err[kk] <- Error
}
@
<<label=Answer2plot1, echo=True,results=verbatim, fig=true>>=
plot(Err)
@
\section*{Question 3}

3) Classify the wine quality data using Naive Bayes. Compare the results with
the two methods described in problem 1 of this homework set. Think about
why one of the methods used works better than the others.

<<label=Setup3, echo=True,results=verbatim>>=
rm(list = ls())
library(klaR)
wine_data <- read.table("winequality-red.csv", header = TRUE, sep = ";")
str(wine_data)
@

\section*{Answer  3}

<<label=Answer3, echo=True,results=verbatim>>=
wine_data$quality <- as.factor(wine_data$quality)
mod <- naiveBayes(quality~.,data = wine_data)
qualityHat <- predict(mod, wine_data[,1:11])

Err <- 1 - sum(qualityHat == wine_data$quality)/length(wine_data$quality)
@
<<label=Answer3plot1, echo=True,results=verbatim, fig=true>>=
pairs(wine_data[,1:11])
@


\section*{Question 4}

4) Classify the sonar data using Naive Bayes. Compare the results with the
methods used in class and with the last homework set. Give reasons for any
discrepancies between the results for these methods. (Either in class or in
homework, the following methods have been used on this data set: Trees,
Linear Regression, Ridge Regression, an Ensemble Method, and now Naive
Bayes.)

<<label=Setup4, echo=True,results=verbatim>>=
rm(list = ls())
library(class)
library(e1071)
library(klaR)
library(MASS)
library(rpart)

sonar.train <- read.csv("sonar_train.csv", header = FALSE)
sonar.train$V61 <- as.factor(sonar.train$V61)
m <- NaiveBayes(V61 ~ ., data = sonar.train)
out <- predict(m)
@

\section*{Answer  4}

<<label=Answer4, echo=True,results=verbatim>>=
Err <- 1 - sum(out$class == sonar.train$V61) / length(sonar.train$V61)
Err
train.labels <- sonar.train$V61
train <- sonar.train[,-61]

Err <- rep(0, 20)
for (kk in seq(from = 1, to = 20)) {
  out <- knn.cv(train, train.labels, k = kk)
  Error <- 1 - sum(abs(train.labels == out)) / length(out)
  Err[kk] <- Error
}
Err
@
<<label=Answer4plot1, echo=True,results=verbatim, fig=true>>=
plot(Err)
@
<<label=Answer4cont1, echo=True,results=verbatim>>=
bestk = which.min(Err)
bestk
@

\section*{Question 5}

5) Run the code in the file KfirstNearestNeighbor.R Does KNN create a better
model if the data is first scaled and normalized? What should be chosen as
the best value for k and why? Now use KNN with cross validation on the
mixtureSimData.data. What is the best value for k for this data?

<<label=Setup5, echo=True,results=verbatim>>=
rm(list = ls())
oldpar <- par(no.readonly = TRUE)
par(mar = rep(1, 4))
library(class)
library(e1071)
STrain <- read.table("sonar_train.csv", sep = ",", header = FALSE)
STest <- read.table("sonar_test.csv", sep = ",", header = FALSE)
@

\section*{Answer  5}

<<label=Answer5, echo=True,results=verbatim>>=
Sonar <- rbind(STrain, STest)
train <- Sonar[, 1:60]
labels <- Sonar[, 61]
# Test error with knn
out <- knn.cv(train, labels, k = 1)
1 - sum(abs(labels == out)) / length(out)
train2 <- train
for (i in seq(from = 1, to = ncol(train))) {
  v = var(train[, i])
  m = mean(train[, i])
  train2[, i] <- (train[, i] - m) / sqrt(v)
}

out <- knn.cv(train2, labels, k = 1)
1 - sum(abs(labels == out)) / length(out)
# Cross - validation

Err <- rep(0, 20)
for (kk in seq(from = 1, to = 20)) {
  out <- knn.cv(train2, labels, k = kk)
  Error <- 1 - sum(abs(labels == out)) / length(out)
  Err[kk] <- Error
}
Err
@
<<label=Answer5plot1, echo=True,results=verbatim, fig=true>>=
plot(Err)
@
<<label=Answer5cont1, echo=True,results=verbatim>>=
## k = 1 gives the best result ----

Err <- rep(0,20)
for(kk in seq(from=1,to=20)){
  out <- knn.cv(train,labels,k=kk)
  Error <- 1-sum(abs(labels == out))/length(out)
  Err[kk] <- Error   
}
Err
@
<<label=Answer5plot2, echo=True,results=verbatim, fig=true>>=
plot(Err)
@
<<label=Answer5cont2, echo=True,results=verbatim>>=
mixSim <- read.table(file="mixtureSimData.data")
mixSimMat <- matrix(0.0,200,2)
mixSimMat[,1] <- mixSim[1:200,1]
mixSimMat[,2] <- mixSim[201:400,1]
Y <- rep(1.0,200)  
Y[101:200] <- 2.0
@
<<label=Answer5plot3, echo=True,results=verbatim, fig=true>>=
plot(mixSimMat)
points(mixSimMat[101:200,1:2], col = 2) # color = Red
points(mixSimMat[1:100,1:2], col = 3)   # color = Green

linMod <- lm(Y~mixSimMat)
coef <- linMod$coefficients
a <- (1.5-coef[1])/coef[3]
b <- -coef[2]/coef[3]
abline(a,b)
@
<<label=Answer5cont3, echo=True,results=verbatim>>=
maxX1 <- max(mixSimMat[,1])
minX1 <- min(mixSimMat[,1])
maxX2 <- max(mixSimMat[,2])
minX2 <- min(mixSimMat[,2])

testMat <- matrix(0.0,10000,2) #matrix(data,number of rows, number of columns)

for(i in 1:100){
  for(j in 1:100){
    x1 <- minX1 + i*(maxX1 - minX1)/100
    x2 <- minX2 + j*(maxX2 - minX2)/100
    index <- (i-1)*100 + j
    testMat[index,1] <- x1
    testMat[index,2] <- x2
  }    
}

XX <- c((1:100)*(maxX1 - minX1))
XX <- XX/100
XX <- XX + minX1

YY <- c((1:100)*(maxX2 - minX2))
YY <- YY/100
YY <- YY + minX2

i = 7
j = 2
index <- (i-1)*100 + j
testMat[index,]
XX[i]
YY[j]

require(class)
KNN <- knn(mixSimMat, testMat, Y, 15)  


ZZ <- matrix(0.0,100,100)
for(i in 1:100){
  for(j in 1:100){
    index <- (i-1)*100 + j
    ZZ[i,j] <- KNN[index]
  }
}

i = 7;j = 2;index <- (i-1)*100 + j
KNN[index] 
ZZ[i,j]    
@
<<label=Answer5plot3, echo=True,results=verbatim, fig=true>>=
I1 <- which(KNN == 1)
# first plot the grid as .  (test data)
# next plot the data as o
plot(testMat, pch=".")
points(testMat[I1,], col=2, cex = 0.2,pch=20)   # plot a small red .
points(testMat[-I1,],col = 3, cex = 0.2,pch=20) # plot a small green .
points(mixSimMat[1:100,], col = 3)
points(mixSimMat[101:200,], col = 2)
contour(XX,YY,ZZ,levels = 1.5, drawlabels = FALSE, add = TRUE)
@
<<label=Answer5plot4, echo=True,results=verbatim, fig=true>>=
KNN <- knn(mixSimMat, testMat, Y, 5)
ZZ <- matrix(0.0, 100, 100)
for (i in 1:100) {
  for (j in 1:100) {
    index <- (i - 1) * 100 + j
    ZZ[i, j] <- KNN[index]
  }
}
I1 <- which(KNN == 1)
plot(testMat, pch = ".")
points(testMat[I1, ],
       col = 2,
       cex = 0.2,
       pch = 20)
points(testMat[-I1, ],
       col = 3,
       cex = 0.2,
       pch = 20)
points(mixSimMat[1:100, ], col = 3)
points(mixSimMat[101:200, ], col = 2)
contour(XX,
        YY,
        ZZ,
        levels = 1.5,
        drawlabels = FALSE,
        add = TRUE)
@
<<label=Answer5plot5, echo=True,results=verbatim, fig=true>>=
KNN <- knn(mixSimMat, testMat, Y, 1)
ZZ <- matrix(0.0, 100, 100)
for (i in 1:100) {
  for (j in 1:100) {
    index <- (i - 1) * 100 + j
    ZZ[i, j] <- KNN[index]
  }
}
I1 <- which(KNN == 1)

plot(testMat, pch = ".")
points(testMat[I1, ],
       col = 2,
       cex = 0.2,
       pch = 20)
points(testMat[-I1, ],
       col = 3,
       cex = 0.2,
       pch = 20)
points(mixSimMat[1:100, ], col = 3)
points(mixSimMat[101:200, ], col = 2)
contour(XX,
        YY,
        ZZ,
        levels = 1.5,
        drawlabels = FALSE,
        add = TRUE)
@
<<label=Answer5cont4, echo=True,results=verbatim>>=
Err <- rep(0,50)
for(kk in seq(from=1,to=50)){
  out <- knn.cv(mixSimMat, Y, k=kk)
  Error <- 1-sum(abs(Y == out))/length(out)
  Err[kk] <- Error   
}
Err

min(Err)  
which(Err == min(Err))
@
<<label=Answer5plot6, echo=True,results=verbatim, fig=true>>=
plot(Err)
@
\end{document}